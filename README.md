---
annotations_creators:
- crowdsourced
language:
- ja
language_creators:
- found
license:
- cc-by-sa-4.0
multilinguality:
- monolingual
pretty_name: JDocQA
size_categories:
- 1K<n<10K
source_datasets:
- original
tags: []
task_categories:
- question-answering
task_ids:
- extractive-qa
- open-domain-qa
- closed-domain-qa
---

# Dataset Card for JDocQA

[![CI](https://github.com/shunk031/huggingface-datasets_JDocQA/actions/workflows/ci.yaml/badge.svg)](https://github.com/shunk031/huggingface-datasets_JDocQA/actions/workflows/ci.yaml)
[![Sync HF](https://github.com/shunk031/huggingface-datasets_JDocQA/actions/workflows/push_to_hub.yaml/badge.svg)](https://github.com/shunk031/huggingface-datasets_JDocQA/actions/workflows/push_to_hub.yaml)
[![LREC-COLING2024 2024.lrec-main.830](https://img.shields.io/badge/LREC--COLING2024-2024.lrec--830-red)](https://aclanthology.org/2024.lrec-main.830/)
[![Hugging Face Datasets Hub](https://img.shields.io/badge/Hugging%20Face_ðŸ¤—-Datasets-ffcc66)](https://huggingface.co/datasets/shunk031/JDocQA)

## Table of Contents
- [Dataset Card Creation Guide](#dataset-card-creation-guide)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
  - [Dataset Creation](#dataset-creation)
    - [Curation Rationale](#curation-rationale)
    - [Source Data](#source-data)
      - [Initial Data Collection and Normalization](#initial-data-collection-and-normalization)
      - [Who are the source language producers?](#who-are-the-source-language-producers)
    - [Annotations](#annotations)
      - [Annotation process](#annotation-process)
      - [Who are the annotators?](#who-are-the-annotators)
    - [Personal and Sensitive Information](#personal-and-sensitive-information)
  - [Considerations for Using the Data](#considerations-for-using-the-data)
    - [Social Impact of Dataset](#social-impact-of-dataset)
    - [Discussion of Biases](#discussion-of-biases)
    - [Other Known Limitations](#other-known-limitations)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)
    - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://github.com/mizuumi/JDocQA
- **Repository:** https://github.com/shunk031/huggingface-datasets_JDocQA
- **Paper (Preprint):** https://arxiv.org/abs/2403.19454
- **Paper (LREC-COLING2014)**: https://aclanthology.org/2024.lrec-main.830/

### Dataset Summary

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> Japanese Document Question Answering (JDocQA), a large-scale document-based QA dataset, essentially requiring both visual and textual information to answer questions, which comprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese.

### Supported Tasks and Leaderboards

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> We consider generative question answering where a model generates a textual answer following the document context and textual question. For realistic applications of a wide range of user questions for documents, we prepare four categories of questions: **(1) yes/no**, **(2) factoid**, **(3) numerical**, and **(4) open-ended**. 
> 
> - In **yes/no questions**, answers are â€œyesâ€ or â€œno.â€ 
> - In **factoid questions**, answers are some facts, such as named entities, that typically appear in the given documents. 
> - In **numerical questions**, answers are numeric values, often including some numerals (some units, e.g., km or Japanese numerals such as â€œ8å€‹ (objects)â€ and â€œ8äºº (persons)â€). These numeric values are written in the documents or are calculated from other numbers in the documents. 
> - In **open-ended questions**, free-form responses are required. For such questions, we aim to assess complex comprehension abilities, such as the ability to form opinions or brief explanations based on the provided contexts and questions. 
> 
> Figure 1 presents samples of these four categories of questions. All examples include diverse images and question types related to some Japanese documents collected. We also include unanswerable questions for each question category.

### Languages

The language data in JDocQA is in Japanese ([BCP-47 ja-JP](https://www.rfc-editor.org/info/bcp47)).

## Dataset Structure

### Data Instances

```python
import datasets as ds

dataset = ds.load_dataset(
  path="shunk031/JDocQA", 
  # Rename to the same wording as in the paper: Document -> Report / Kouhou -> Pamphlet
  rename_pdf_category=True,
  # Set to True to use loading script for huggingface datasets
  trust_remote_code=True,
)

print(dataset)
# DatasetDict({
#     train: Dataset({
#         features: ['answer', 'answer_type', 'context', 'multiple_select_answer', 'multiple_select_question', 'no_reason', 'normalized_answer', 'original_answer', 'original_context', 'original_question', 'pdf_category', 'pdf_name', 'question', 'question_number', 'question_page_number', 'reason_of_answer_bbox', 'text_from_ocr_pdf', 'text_from_pdf', 'type_of_image', 'pdf_filepath'],
#         num_rows: 9290
#     })
#     validation: Dataset({
#         features: ['answer', 'answer_type', 'context', 'multiple_select_answer', 'multiple_select_question', 'no_reason', 'normalized_answer', 'original_answer', 'original_context', 'original_question', 'pdf_category', 'pdf_name', 'question', 'question_number', 'question_page_number', 'reason_of_answer_bbox', 'text_from_ocr_pdf', 'text_from_pdf', 'type_of_image', 'pdf_filepath'],
#         num_rows: 1134
#     })
#     test: Dataset({
#         features: ['answer', 'answer_type', 'context', 'multiple_select_answer', 'multiple_select_question', 'no_reason', 'normalized_answer', 'original_answer', 'original_context', 'original_question', 'pdf_category', 'pdf_name', 'question', 'question_number', 'question_page_number', 'reason_of_answer_bbox', 'text_from_ocr_pdf', 'text_from_pdf', 'type_of_image', 'pdf_filepath'],
#         num_rows: 1176
#     })
# })
```

An example of the JDocQA dataset (training set) looks as follows:

```json
{
    "answer": "æœ¬æ–‡ä¸­ã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“",
    "answer_type": 3,
    "context": "_II.èª¿æŸ»å†…å®¹(2.è™¹æœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®ç­–å®š)(3)åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®è¨˜è¼‰é …ç›®å‰è¿°ã®æ–¹é‡ç­‰ã‚’è¸ã¾ãˆã€åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®å…·ä½“çš„ãªè¨˜è¼‰é …ç›®(ç›®æ¬¡ã ã¦)ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã¨ã™ã‚‹ã€‚å°é …ç›®ãƒ»å†…å®¹Iã¯ã˜ã‚ã«ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®ç›®çš„ã€ç«‹ä¼šç¾©å‹™_(æ¶ˆé˜²æ³•ç¬¬13æ¡ç¬¬3é …)å®‰å…¨å¯¾ç­–ã®åŸºæœ¬äº‹é …(SSç«‹ä¼šã„è€…ã¨ãƒ­ãƒ¼ãƒªãƒ¼ä¹—å‹™å“¡ã«ã‚ˆã‚‹ç›¸äº’ç¢ºèªãƒ»ç›¸äº’å”åŠ›ã®é‡è¦æ€§)ãƒ­ãƒ¼ãƒªãƒ¼è·å¸ã—ã®æ‰‹é †ã®åŸºæœ¬çš„æµã‚Œ â€»è©³ç´°ç‰ˆã®ã¿IIãƒ­ãƒ¼ãƒªãƒ¼è·é‚Šã—æ™‚ã®ä½œæ¥­å†…å®¹1ãƒ­ãƒ¼ãƒªãƒ¼åˆ°ç€æ™‚(è·çˆºã—å‰)1.ãƒ­ãƒ¼ãƒªãƒ¼åœè»Šä½ç½®ã®ç¢ºèª:è¨ˆå°Ž2.ç´å“æ›¸ã®ç›¸äº’ç¢ºèª3.ã‚¢ãƒ¼ã‚¹ã®æŽ¥ç¶š4.æ¶ˆç«å™¨ã®é…ç½®5.ç©è·ã®ç›¸äº’ç¢ºèª6.åœ°ä¸‹ã‚¿ãƒ³ã‚¯å’Œåœ¨åº«åŠã³å’Œè·å¸ã—æ•°é‡ã®ç¢ºèª7ãƒ»è©³ç´°ç‰ˆã«ã¯ã€å„é …ç›®ã”ã¨ã«ã€_-SSç«‹ä¼šã„è€…ã€ãƒ­ãƒ¼ãƒªãƒ¼ä¹—å‹™2è·é‚Šã—æ™‚(ãƒ›ãƒ¼ã‚¹ã®çµåˆ)03-.æ³¨æ²¹å£ã®ç¢ºèªã€ãƒ›ãƒ¼ã‚¹ã®çµåˆã‚’è¨˜è¼‰3.ãƒ™ãƒ¼ãƒ‘ãƒ¼å›žåŽãƒ›ãƒ¼ã‚¹æŽ¥ç¶šee4è·å¸ã—ä½œæ¥­ä¸­ã®å®‰å…¨é¦¬è¦–ç‰¹ã«é‡è¦ãªåŸºæœ¬äº‹3è·å¸ã—çµ‚äº†æ™‚1.é…ç®¡å†…ã€ãƒ›ãƒ¼ã‚¹å†…ã®æ®‹æ²¹ã®ç¢ºèª2.æ³¨æ²¹å£ã®ç¢ºèªãƒãƒƒãƒå†…æ®‹æ²¹ç¢ºèª3.åœ¨åº«ç¢ºèª4.5.å¾Œç‰‡ä»˜ã‘6.ãƒ­ãƒ¼ãƒªãƒ¼ã®é€€å‡ºè‡ªäº‹æ•…ãƒ»ç½å®³æ™‚ã®å¯¾å‡¦(åˆå‹•å¯¾å¿œ)1ã‚³ãƒ³ã‚¿ãƒŸ(æ··æ²¹)äº‹æ•…ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€é€£çµ¡2ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼(æ¼æ²¹)äº‹æ•…ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€é€£çµ¡3ç«ç½ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€åˆæœŸæ¶ˆç«IVé€šå ±ãƒ»ç·Šæ€¥é€£çµ¡ç·Šæ€¥æ™‚é€£çµ¡å…ˆã€é€šå ±å†…å®¹å‚è€ƒãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆä¾‹",
    "multiple_select_answer": 3,
    "multiple_select_question": ["ã¯ã„", "ã„ã„ãˆ", "ã‚ã‹ã‚‰ãªã„", "æœ¬æ–‡ä¸­ã«è¨˜è¼‰ãŒè¦‹ã¤ã‘ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ"],
    "no_reason": 0,
    "normalized_answer": "æœ¬æ–‡ä¸­ã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“",
    "original_answer": "æœ¬æ–‡ä¸­ã«è¨˜è¼‰ãŒè¦‹ã¤ã‘ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ",
    "original_context": "_II.èª¿æŸ»å†…å®¹(2.è™¹æœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®ç­–å®š)(3)åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®è¨˜è¼‰é …ç›®å‰è¿°ã®æ–¹é‡ç­‰ã‚’è¸ã¾ãˆã€åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®å…·ä½“çš„ãªè¨˜è¼‰é …ç›®(ç›®æ¬¡ã ã¦)ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã¨ã™ã‚‹ã€‚å°é …ç›®ãƒ»å†…å®¹Iã¯ã˜ã‚ã«ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®ç›®çš„ã€ç«‹ä¼šç¾©å‹™_(æ¶ˆé˜²æ³•ç¬¬13æ¡ç¬¬3é …)å®‰å…¨å¯¾ç­–ã®åŸºæœ¬äº‹é …(SSç«‹ä¼šã„è€…ã¨ãƒ­ãƒ¼ãƒªãƒ¼ä¹—å‹™å“¡ã«ã‚ˆã‚‹ç›¸äº’ç¢ºèªãƒ»ç›¸äº’å”åŠ›ã®é‡è¦æ€§)ãƒ­ãƒ¼ãƒªãƒ¼è·å¸ã—ã®æ‰‹é †ã®åŸºæœ¬çš„æµã‚Œ â€»è©³ç´°ç‰ˆã®ã¿IIãƒ­ãƒ¼ãƒªãƒ¼è·é‚Šã—æ™‚ã®ä½œæ¥­å†…å®¹1ãƒ­ãƒ¼ãƒªãƒ¼åˆ°ç€æ™‚(è·çˆºã—å‰)1.ãƒ­ãƒ¼ãƒªãƒ¼åœè»Šä½ç½®ã®ç¢ºèª:è¨ˆå°Ž2.ç´å“æ›¸ã®ç›¸äº’ç¢ºèª3.ã‚¢ãƒ¼ã‚¹ã®æŽ¥ç¶š4.æ¶ˆç«å™¨ã®é…ç½®5.ç©è·ã®ç›¸äº’ç¢ºèª6.åœ°ä¸‹ã‚¿ãƒ³ã‚¯å’Œåœ¨åº«åŠã³å’Œè·å¸ã—æ•°é‡ã®ç¢ºèª7ãƒ»è©³ç´°ç‰ˆã«ã¯ã€å„é …ç›®ã”ã¨ã«ã€_-SSç«‹ä¼šã„è€…ã€ãƒ­ãƒ¼ãƒªãƒ¼ä¹—å‹™2è·é‚Šã—æ™‚(ãƒ›ãƒ¼ã‚¹ã®çµåˆ)03-.æ³¨æ²¹å£ã®ç¢ºèªã€ãƒ›ãƒ¼ã‚¹ã®çµåˆã‚’è¨˜è¼‰3.ãƒ™ãƒ¼ãƒ‘ãƒ¼å›žåŽãƒ›ãƒ¼ã‚¹æŽ¥ç¶šee4è·å¸ã—ä½œæ¥­ä¸­ã®å®‰å…¨é¦¬è¦–ç‰¹ã«é‡è¦ãªåŸºæœ¬äº‹3 è·å¸ã—çµ‚äº†æ™‚1.é…ç®¡å†…ã€ãƒ›ãƒ¼ã‚¹å†…ã®æ®‹æ²¹ã®ç¢ºèª2.æ³¨æ²¹å£ã®ç¢ºèªãƒãƒƒãƒå†…æ®‹æ²¹ç¢ºèª3.åœ¨åº«ç¢ºèª4.5.å¾Œç‰‡ä»˜ã‘6.ãƒ­ãƒ¼ãƒªãƒ¼ã®é€€å‡ºè‡ªäº‹æ•…ãƒ»ç½å®³æ™‚ã®å¯¾å‡¦(åˆå‹•å¯¾å¿œ)1ã‚³ãƒ³ã‚¿ãƒŸ(æ··æ²¹)äº‹æ•…ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€é€£çµ¡2ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼(æ¼æ²¹)äº‹æ•…ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€é€£çµ¡3ç«ç½ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€åˆæœŸæ¶ˆç«IVé€šå ±ãƒ»ç·Šæ€¥é€£çµ¡ç·Šæ€¥æ™‚é€£çµ¡å…ˆã€é€šå ±å†…å®¹å‚è€ƒãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆä¾‹",
    "original_question": "åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®å…·ä½“çš„ãªè¨˜è¼‰é …ç›®ã¨ã—ã¦ã„ã‚‹äº‹æ•…ãƒ»ç½å®³æ™‚ã®å¯¾å‡¦ã®ä¸­ã§ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ï¼ˆæ¼æ²¹ï¼‰äº‹æ•…ãŒèµ·ã“ã£ãŸå ´åˆã¯ç™ºè¦‹æ™‚ã«ã©ã®ã‚ˆã†ãªå‡¦ç½®ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ",
    "pdf_category": 2,
    "pdf_name": "public_document00152.pdf",
    "question": "åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®å…·ä½“çš„ãªè¨˜è¼‰é …ç›®ã¨ã—ã¦ã„ã‚‹äº‹æ•…ãƒ»ç½å®³æ™‚ã®å¯¾å‡¦ã®ä¸­ã§ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ï¼ˆæ¼æ²¹ï¼‰äº‹æ•…ãŒèµ·ã“ã£ãŸå ´åˆã¯ç™ºè¦‹æ™‚ã«ã©ã®ã‚ˆã†ãªå‡¦ç½®ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ\nè§£ç­”ã¯è‡ªç”±ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚",
    "question_number": [4, 656, 1, 4],
    "question_page_number": "9",
    "reason_of_answer_bbox": [""],
    "text_from_ocr_pdf": "_II.èª¿æŸ»å†…å®¹(2.è™¹æœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®ç­–å®š)(3)åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®è¨˜è¼‰é …ç›®å‰è¿°ã®æ–¹é‡ç­‰ã‚’è¸ã¾ãˆã€åŸºæœ¬ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®å…·ä½“çš„ãªè¨˜è¼‰é …ç›®(ç›®æ¬¡ã ã¦)ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã¨ã™ã‚‹ã€‚å°é …ç›®ãƒ»å†…å®¹Iã¯ã˜ã‚ã«ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã®ç›®çš„ã€ç«‹ä¼šç¾©å‹™_(æ¶ˆé˜²æ³•ç¬¬13æ¡ç¬¬3é …)å®‰å…¨å¯¾ç­–ã®åŸºæœ¬äº‹é …(SSç«‹ä¼šã„è€…ã¨ãƒ­ãƒ¼ãƒªãƒ¼ä¹—å‹™å“¡ã«ã‚ˆã‚‹ç›¸äº’ç¢ºèªãƒ»ç›¸äº’å”åŠ›ã®é‡è¦æ€§)ãƒ­ãƒ¼ãƒªãƒ¼è·å¸ã—ã®æ‰‹é †ã®åŸºæœ¬çš„æµã‚Œ â€»è©³ç´°ç‰ˆã®ã¿IIãƒ­ãƒ¼ãƒªãƒ¼è·é‚Šã—æ™‚ã®ä½œæ¥­å†…å®¹1ãƒ­ãƒ¼ãƒªãƒ¼åˆ°ç€æ™‚(è·çˆºã—å‰)1.ãƒ­ãƒ¼ãƒªãƒ¼åœè»Šä½ç½®ã®ç¢ºèª:è¨ˆå°Ž2.ç´å“æ›¸ã®ç›¸äº’ç¢ºèª3.ã‚¢ãƒ¼ã‚¹ã®æŽ¥ç¶š4.æ¶ˆç«å™¨ã®é…ç½®5.ç©è·ã®ç›¸äº’ç¢ºèª6.åœ°ä¸‹ã‚¿ãƒ³ã‚¯å’Œåœ¨åº«åŠã³å’Œè·å¸ã—æ•°é‡ã®ç¢ºèª7ãƒ»è©³ç´°ç‰ˆã«ã¯ã€å„é …ç›®ã”ã¨ã«ã€_-SSç«‹ä¼šã„è€…ã€ãƒ­ãƒ¼ãƒªãƒ¼ä¹—å‹™2è·é‚Šã—æ™‚(ãƒ›ãƒ¼ã‚¹ã®çµåˆ)03-.æ³¨æ²¹å£ã®ç¢ºèªã€ãƒ›ãƒ¼ã‚¹ã®çµåˆã‚’è¨˜è¼‰3.ãƒ™ãƒ¼ãƒ‘ãƒ¼å›žåŽãƒ›ãƒ¼ã‚¹æŽ¥ç¶šee4è·å¸ã—ä½œæ¥­ä¸­ã®å®‰å…¨é¦¬è¦–ç‰¹ã«é‡è¦ãªåŸºæœ¬äº‹3è·å¸ã—çµ‚äº†æ™‚1.é…ç®¡å†…ã€ãƒ›ãƒ¼ã‚¹å†…ã®æ®‹æ²¹ã®ç¢ºèª2.æ³¨æ²¹å£ã®ç¢ºèªãƒãƒƒãƒå†…æ®‹æ²¹ç¢ºèª3.åœ¨åº«ç¢ºèª4.5.å¾Œç‰‡ä»˜ã‘6.ãƒ­ãƒ¼ãƒªãƒ¼ã®é€€å‡ºè‡ªäº‹æ•…ãƒ»ç½å®³æ™‚ã®å¯¾å‡¦(åˆå‹•å¯¾å¿œ)1ã‚³ãƒ³ã‚¿ãƒŸ(æ··æ²¹)äº‹æ•…ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€é€£çµ¡2ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼(æ¼æ²¹)äº‹æ•…ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€é€£çµ¡3ç«ç½ç™ºè¦‹æ™‚(ç·Šæ€¥å‡¦ç½®)ã€åˆæœŸæ¶ˆç«IVé€šå ±ãƒ»ç·Šæ€¥é€£çµ¡ç·Šæ€¥æ™‚é€£çµ¡å…ˆã€é€šå ±å†…å®¹å‚è€ƒãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆä¾‹",
    "text_from_pdf": "",
    "type_of_image": [0],
    "pdf_filepath": "/home/shunk031/.cache/huggingface/datasets/downloads/extracted/f3481b9f65c75efec1e5398f76bd8347e64661573961b69423568699f1d7083a/pdf_files/public_document00152.pdf"
}
```

### Data Fields

From [JDocQA's README.md](https://github.com/mizuumi/JDocQA/blob/main/dataset/README.md) and [the paper](https://arxiv.org/abs/2403.19454):

- `answer`: 
- `answer_type`: (1) Yes/No questions, (2) Factoid questions, (3) Numerical questions, (4) Open-ended questions.
- `context`: Removed noises from 'original_context'.
- `multiple_select_answer`:
- `multiple_select_question`:
- `no_reason`: Unanswerable question -> 0, Answerable question -> 1, Multi page question -> 2. They can be jointly flagged such as `1,2`.
- `normalized_answer`:
- `original_answer`: Annotated answers.
- `original_context`: Extracted texts from PDF.
- `original_question`: Annotated questions.
- `pdf_category`: Document category.
- `pdf_name`: PDF name.
- `question`: Question query for models.
- `question_number`:
- `question_page_number`: Where annotators found answer of the questions.
- `reason_of_answer_bbox`:
- `text_from_ocr_pdf`:
- `text_from_pdf`:
- `type_of_image`: (1) Table, (2) Bar chart, (3) Line chart, (4) Pie chart, (5) Map, (6) Other figures, (7) Mixtured writing style from left to the right and from upside to the downside, (8) Drawings, (9) Others. Note that this enrty is for statistical purpose in our paper, and some labels are missing, which are represented as `null`.
- `pdf_filepath`: full file path to the corresponding PDF file.

> ## pdf_category
> We renamed the several category names upon the paper for the interpretability.
> - `Document` category in the PDF set as `Report` in the paper.
> - `Kouhou` category in the PDF set as `Pamphlet` in the paper.

### Data Splits

From [JDocQA's paper](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/C3-5.pdf):

> å­¦ç¿’ï¼Œæ¤œå®šï¼Œãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«ãã‚Œãžã‚Œ 9,290 ä»¶ï¼Œ1,134 ä»¶ï¼Œ1,176 ä»¶ã®è³ªå•å¿œç­”ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’åˆ†å‰²ã—ãŸï¼ŽåŒä¸€ PDF ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¿…ãšåŒä¸€ã®åˆ†å‰²ã«å‡ºç¾ã™ã‚‹ï¼Ž

## Dataset Creation

### Curation Rationale

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> To address the demand for a large-scale and fully annotated Japanese document question answering dataset, we introduce a JDocQA dataset by collecting Japanese documents in PDF styles from open-access sources including multiple formats of documents: slides, reports, websites and pamphlets and manually annotating question-answer pairs on them.

### Source Data

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> We gather public documents, such as, municipality pamphlets and websites, that are created by Japanese governmental agencies or local governments. 

#### Initial Data Collection and Normalization

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> We manually collected PDF documents from open-access resources such as Japanese National Diet Library (NDL)â€™s digital collection, web archive projects (WARP) and websites of Japanese government ministries. We manually gathered documents such as reports, pamphlets or websites that are published by public or quasi-public sectors, such as local governments or public universities through WARP. We also gather Japanese ministry documents such as slides and reports from their websites following the government agenciesâ€™ policies. Those documents cover a wide range of topics, for instance, economic policies, education policies, labor issues, health and hygiene, agriculture, forestry, fisheries, culture and arts, history, related to governmental policy or policy guidelines, as well as the everyday affairs of local governments. These documents also include visual elements such as figures, tables, charts, pictures, or mandala charts, complex figures with a combination of texts and objects typically seen in the Japanese public administrative sectorâ€™s official document. We classify these documents into four categories, namely, pamphlet, slide, report, and website considering the form of the documents.

> We extracted texts from PDF documents with PyPDF2. We also notice that some PDF documents are probably created from paper scans, and we cannot extract embedded texts from such documents. Therefore, we extracted texts from the document page images by OCR (Optical Character Recognition) as an alternative source. After the text extraction or OCR, we removed mistakenly recognized symbols and emojis, or duplicated characters from texts when the same character continuously and repeatedly appeared more than five times.

#### Who are the source language producers?

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> JDocQA dataset comprises 5,504 files and 11,600 question-and-answer pairs in Japanese.

### Annotations

#### Annotation process

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> As documents include rich textual and visual elements (e.g., graphs, charts, maps, illustrations, and a mix of vertical and horizontal written text), we made question answer pairs that are related to both textual and visual information. We ask annotators to write up two to four question-answer annotations in each document. We also ask not to use any AI-tools such as OpenAI ChatGPT during the annotation process. Each question is accompanied with the supporting facts as marked in red in Figure 1 and Figure 3. We classify a subset of questions that have multiple supporting facts in multiple pages as multi-page questions. Multi-page questions are considerably difficult from their single-page counterparts. For unanswerable questions, we ask annotators to write questions that lack supporting facts in the documents, making them impossible to answer based on the given documents.

> We prepared three types of images for visual inputs for multimodal models. The first type of images are those of the whole page of the documents including the annotated question answering pairs. The second type of images are those cropped by bounding boxes on which annotators based their answers such as tables or figures of the pages. When multiple bounding boxes are annotated to a single question-answer pair, multiple cropped images are combined together into a single image here. The third type of images are blank (white) images that are used for ablation studies.

#### Who are the annotators?

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> We ask 43 annotators in total for the question-answering pairs annotation on documents.

### Personal and Sensitive Information

[More Information Needed]

<!-- State whether the dataset uses identity categories and, if so, how the information is used. Describe where this information comes from (i.e. self-reporting, collecting from profiles, inferring, etc.). See [Larson 2017](https://www.aclweb.org/anthology/W17-1601.pdf) for using identity categories as a variables, particularly gender. State whether the data is linked to individuals and whether those individuals can be identified in the dataset, either directly or indirectly (i.e., in combination with other data).

State whether the dataset contains other data that might be considered sensitive (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history).  

If efforts were made to anonymize the data, describe the anonymization process. -->

## Considerations for Using the Data

### Social Impact of Dataset

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> We assume our datasets are useful for both research and development of generative language models and their applications for Japanese document question answering. 

### Discussion of Biases

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> We carefully avoid private documents and choose considerably public documents published by public or quasi-public sectors for the publicity of our dataset usage. All of the documents and webpages are publicly available online and we follow our institutional rules to gather them. We follow our institutional rules and also consult external advisors for data collection processes.

### Other Known Limitations

From [JDocQA's paper](https://arxiv.org/abs/2403.19454):

> We also consider our dataset with unanswerable questions can contribute to harnessing the hallucination problem of large language models. However, this doesnâ€™t mean that the fintuned models with unanswerable questions do not perform hallucinations at all.

## Additional Information

### Dataset Curators

[More Information Needed]

<!-- List the people involved in collecting the dataset and their affiliation(s). If funding information is known, include it here. -->

### Licensing Information

From [JDocQA's README.md](https://github.com/mizuumi/JDocQA/blob/main/dataset/README.md):

> JDocQA dataset annotations are distributed under CC BY-SA 4.0. We are delighted to see many derivations from JDocQA! When you create any derivations, e.g., datasets, papers, etc, from JDocQA, please cite our paper accordingly. If your derivations are web-based projects, please cite our paper and include the link to [this github page](https://github.com/mizuumi/JDocQA?tab=readme-ov-file#cite).

### Citation Information

```bibtex
@inproceedings{onami2024jdocqa,
  title={JDocQA: Japanese Document Question Answering Dataset for Generative Language Models},
  author={Onami, Eri and Kurita, Shuhei and Miyanishi, Taiki and Watanabe, Taro},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={9503--9514},
  year={2024}
}
```

### Contributions

Thanks to [@mizuumi](https://github.com/mizuumi) for creating this dataset.
